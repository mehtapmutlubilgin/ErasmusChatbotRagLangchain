import os
import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
# HuggingFace (Yerel) embedding modelini kullanÄ±yoruz
from langchain_community.embeddings import HuggingFaceEmbeddings 
# Generation iÃ§in Gemini kullanÄ±yoruz
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.runnables import RunnablePassthrough
from operator import itemgetter
from dotenv import load_dotenv

# 1. Sabitler ve Kurulum
load_dotenv() 

# setup_db.py dosyasÄ±nda oluÅŸturduÄŸumuz yerel veritabanÄ± adÄ±
CHROMA_PATH = "chroma_db_local" 
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2" # Yerel model
LLM_MODEL = "gemini-2.5-flash" 
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    st.error("LÃ¼tfen GEMINI_API_KEY ortam deÄŸiÅŸkenini ayarlayÄ±n (Ã¶rn: set komutuyla).")
    st.stop()

# 2. RAG BileÅŸenlerini YÃ¼kleme ve BaÅŸlatma
# Caching (st.cache_resource) sorunlarÄ±nÄ± Ã¶nlemek iÃ§in kaldÄ±rÄ±ldÄ±.
def get_rag_chain(api_key):
    # Embedding Modeli (HuggingFace yerel modelini kullanÄ±yoruz)
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
    try:
        # VektÃ¶r depoyu yÃ¼kle
        vector_store = Chroma(
            persist_directory=CHROMA_PATH,
            embedding_function=embeddings
        )
        # Retriever (Geri Getirici) oluÅŸtur
        retriever = vector_store.as_retriever(search_kwargs={"k": 3}) 
    except Exception as e:
        st.error(f"VektÃ¶r veritabanÄ± yÃ¼klenirken kritik hata: {e}")
        st.stop()
        
    # LLM (Large Language Model) - Gemini Generation
    llm = ChatGoogleGenerativeAI(
        model=LLM_MODEL, 
        temperature=0.4,
        google_api_key=api_key,
        verbose=True  # Terminalde zincir adÄ±mlarÄ±nÄ± gÃ¶rmemizi saÄŸlar
    )

    # Prompt Åablonu
    template = """Sen bir Erasmus programÄ± asistanÄ±sÄ±n. Sadece verilen baÄŸlamÄ± kullanarak, kullanÄ±cÄ±nÄ±n sorusuna kibar ve doÄŸru bir yanÄ±t ver.
    EÄŸer baÄŸlamda yeterli bilgi yoksa, 'Bu konuda elimde yeterli bilgi yok.' veya 'Sadece Erasmus programÄ± ile ilgili sorulara yanÄ±t verebilirim.' diye yanÄ±tla.
    
    BaÄŸlam (Context):
    {context}
    
    Soru (Question):
    {question}"""
    
    prompt = ChatPromptTemplate.from_template(template)

    # RAG Zinciri iÃ§in yardÄ±mcÄ± fonksiyon
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # Cevap oluÅŸturma zinciri
    rag_chain_from_docs = (
        {
            "context": itemgetter("question") | retriever | format_docs,
            "question": itemgetter("question"),
        }
        | prompt
        | llm
    )
    
    # Ana RAG zinciri (Cevap ve kaynak dokÃ¼manlarÄ± dÃ¶ndÃ¼rmek iÃ§in)
    rag_chain_with_source = RunnablePassthrough.assign(
        context=itemgetter("question") | retriever
    ) | {
        "answer": rag_chain_from_docs,
        "docs": itemgetter("context")
    }

    return rag_chain_with_source

# 3. Streamlit ArayÃ¼zÃ¼
st.set_page_config(page_title="Erasmus RAG Chatbot", layout="wide")
st.title("ğŸ‡ªğŸ‡º Erasmus Bilgi AsistanÄ±")
st.caption("LangChain, ChromaDB (Yerel Embedding) ve Gemini ile oluÅŸturulmuÅŸ RAG tabanlÄ± chatbot")

if "messages" not in st.session_state:
    st.session_state.messages = []

# Sohbet geÃ§miÅŸini gÃ¶ster
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# RAG zincirini yÃ¼kle
rag_chain = get_rag_chain(GEMINI_API_KEY)

# KullanÄ±cÄ± giriÅŸi
if prompt := st.chat_input("Erasmus ile ilgili bir soru sorun..."):
    # KullanÄ±cÄ± mesajÄ±nÄ± ekle
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Asistan yanÄ±tÄ±nÄ± oluÅŸtur ve ekle
    with st.spinner("Asistan yanÄ±tÄ± hazÄ±rlanÄ±yor..."):
        try:
            # RAG Zincirini Ã§aÄŸÄ±r
            response = rag_chain.invoke({"question": prompt})

            # YanÄ±tÄ± ve kaynaklarÄ± Ã§ekme
            answer = response['answer'].content
            
            # KaynaklarÄ± formatla
            sources = []
            for i, doc in enumerate(response['docs']):
                kategori = doc.metadata.get('kategori', 'N/A')
                soru = doc.metadata.get('soru', 'N/A')
                cevap = doc.metadata.get('cevap', doc.page_content)
                
                # Kaynak metnini 'cevap' sÃ¼tunundan alÄ±yoruz.
                sources.append(f"**Kaynak {i+1}** (Kategori: {kategori}, Soru: {soru}): {cevap}")
            
            sources_text = "\n\n**BaÄŸlamda KullanÄ±lan Kaynaklar:**\n" + "\n\n".join(sources)
            
            full_response = answer + "\n\n---\n" + sources_text

            # Asistan yanÄ±tÄ±nÄ± gÃ¶ster ve geÃ§miÅŸe kaydet
            with st.chat_message("assistant"):
                st.markdown(answer)
                with st.expander("KullanÄ±lan KaynaklarÄ± GÃ¶r"):
                    st.markdown(sources_text)
                
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
             st.error(f"YanÄ±t oluÅŸturulurken bir hata oluÅŸtu. Hata: {e}")
             st.session_state.messages.append({"role": "assistant", "content": f"Hata: {e}"})